# Integrating the ML model into ROS Nodes
---
Now that we have set up the basic structure of the system with a publisher and receiver node, we can proceed with integrating our ML model for image analysis into these nodes to publish the models output in real-time. We'll start with integrating the model into the image subscriber node and then move on to handling image saving with the correct naming conventions.

### **Step 1: Modify the Image Subscriber Node to Include the ML Model**

We'll first modify the `image_subscriber.py` to include the ML model and process the images as they are received. Here's how to do it:

#### **1.1. Update the `image_subscriber.py` Script**

1. **Navigate to the Package Directory**:
   - Go to the directory where the package is located:

   ```bash
   cd ~/ros2_ws_test/src/image_processor_test/image_processor_test/scripts
   ```

2. **Open the `image_subscriber.py` Script**:
   - Open the `image_subscriber.py` file with a text editor:

   ```bash
   gedit image_subscriber.py
   ```

3. **Replace the Current Script with the Following Code below**:
   - This code integrates the ML model, handles the processing of images, and saves them into subfolders as seen in the directory structure below:

### **Directory Tree Structure:**

Let's assume that the images are being saved in the directory `/home/User/Downloads/Files/new_images/` (you can chose whichever directory you want the models' output to be saved into and specify that directory in the script below), the structure would look something like this:

```
/home/User/Downloads/Files/new_images/
│
├── IMG_1234_results/
│   ├── IMG_1234_output_1.png    # Original Image
│   ├── IMG_1234_output_2.png    # Mask Itself
│   ├── IMG_1234_output_3.png    # Overlayed Image
│   └── IMG_1234_output_4.png    # Mask with Measurements
│
├── IMG_5678_results/
│   ├── IMG_5678_output_1.png    # Original Image
│   ├── IMG_5678_output_2.png    # Mask Itself
│   ├── IMG_5678_output_3.png    # Overlayed Image
│   └── IMG_5678_output_4.png    # Mask with Measurements
│
└── ... (more results folders)
```

### **Explanation:**

- Each image processed by the model results in a subfolder being created inside the `new_images` directory.
- The name of each subfolder is based on the original image filename with `_results` appended to it.
- Inside each subfolder, there are four output images:
  - **`output_1.png`:** The original image.
  - **`output_2.png`:** The mask generated by the model.
  - **`output_3.png`:** The original image with the mask overlaid.
  - **`output_4.png`:** The mask with layer height measurements displayed.

This directory structure should help in organizing and locating the results for each image processed by the model. 

**Here is the actual script for the `image_subscriber.py` node:**

   ```python
   import os
   import cv2
   import numpy as np
   import rclpy
   from rclpy.node import Node
   from sensor_msgs.msg import Image
   from cv_bridge import CvBridge
   import tensorflow as tf
   from tensorflow.keras.models import load_model
   import time

   class ImageSubscriber(Node):
       def __init__(self):
           super().__init__('image_subscriber')
           self.subscription = self.create_subscription(
               Image,
               'image_topic',
               self.listener_callback,
               10)
           self.subscription  
           self.bridge = CvBridge()

           # Load the pre-trained model
           model_path = '/home/User/Downloads/Files/SegmentationLabwithcrop.keras' #change based on where you saved your model
           custom_objects = {
               'dice_loss': self.dice_loss,
               'dice_coefficient': self.dice_coefficient,
               'iou': self.iou
           }
           self.model = load_model(model_path, custom_objects=custom_objects)

           # Directory to save output images
           self.output_dir = '/path/to/save/output/images'  # Update with your desired directory

       @tf.keras.utils.register_keras_serializable()
       def dice_loss(self, y_true, y_pred):
           smooth = 1.
           y_true_f = tf.keras.backend.flatten(y_true)
           y_pred_f = tf.keras.backend.flatten(y_pred)
           intersection = tf.keras.backend.sum(y_true_f * y_pred_f)
           return 1 - (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)

       @tf.keras.utils.register_keras_serializable()
       def dice_coefficient(self, y_true, y_pred):
           smooth = 1.
           y_true_f = tf.keras.backend.flatten(y_true)
           y_pred_f = tf.keras.backend.flatten(y_pred)
           intersection = tf.keras.backend.sum(y_true_f * y_pred_f)
           return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)

       @tf.keras.utils.register_keras_serializable()
       def iou(self, y_true, y_pred):
           smooth = 1.
           y_true_f = tf.keras.backend.flatten(y_true)
           y_pred_f = tf.keras.backend.flatten(y_pred)
           intersection = tf.keras.backend.sum(y_true_f * y_pred_f)
           union = tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) - intersection
           return (intersection + smooth) / (union + smooth)

       def preprocess_image(self, cv_image):
           gray_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
           clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
           enhanced_image = clahe.apply(gray_image)
           blurred_image = cv2.GaussianBlur(enhanced_image, (5, 5), 0)
           edges = cv2.Canny(blurred_image, 50, 150)
           kernel = np.ones((3, 3), np.uint8)
           dilated_edges = cv2.dilate(edges, kernel, iterations=1)
           eroded_edges = cv2.erode(dilated_edges, kernel, iterations=1)

           normalized_image = cv_image / 255.0
           normalized_image = np.expand_dims(normalized_image, axis=0)

           return normalized_image

       def save_images(self, base_filename, images):
           # Create a subfolder for each image's results
           subfolder = os.path.join(self.output_dir, f"{base_filename}_results")
           os.makedirs(subfolder, exist_ok=True)

           for idx, image in enumerate(images):
               save_path = os.path.join(subfolder, f"{base_filename}_output_{idx + 1}.png")
               cv2.imwrite(save_path, image)
               self.get_logger().info(f"Saved {save_path}")

       def listener_callback(self, msg):
           self.get_logger().info('Receiving image')

           # Convert ROS Image message to OpenCV image
           cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")

           # Preprocess the image
           preprocessed_image = self.preprocess_image(cv_image)

           # Perform model inference
           start_time = time.perf_counter()
           predicted_mask = self.model.predict(preprocessed_image)
           end_time = time.perf_counter()
           elapsed_time = end_time - start_time
           self.get_logger().info(f"Model prediction took {elapsed_time:.8f} seconds.")

           # Post-process the prediction
           predicted_mask = (predicted_mask > 0.5).astype(np.uint8)

           # Generate a base filename from the original image name (if available)
           base_filename = msg.header.frame_id.split('/')[-1].split('.')[0]

           # Save the images to the respective subfolder
           self.save_images(base_filename, [
               cv_image,  # Original image
               predicted_mask.squeeze(),  # Mask itself
               self.get_colored_overlay(cv_image, predicted_mask),  # Overlayed image
               self.get_mask_with_measurements(predicted_mask, self.calculate_layer_heights(predicted_mask))  # Mask with measurements
           ])

       def calculate_layer_heights(self, predicted_mask, region_width=40, scaling_factor=2):
           measurement_points = []
           scaled_mask = cv2.resize(predicted_mask.squeeze(), (predicted_mask.shape[2] * scaling_factor, predicted_mask.shape[1] * scaling_factor), interpolation=cv2.INTER_LINEAR)
           scaled_mask = cv2.GaussianBlur(scaled_mask, (7, 7), 0)
           contours, _ = cv2.findContours(scaled_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
           contours = sorted(contours, key=lambda c: cv2.boundingRect(c)[1])

           num_layers = len(contours) - 1
           self.get_logger().info(f"Identified {num_layers} layers.")

           for i in range(num_layers):
               top_contour = contours[i]
               bottom_contour = contours[i+1]

               top_points = [pt[0] for pt in top_contour]
               bottom_points = [pt[0] for pt in bottom_contour]

               for x in range(0, scaled_mask.shape[1], region_width * scaling_factor):
                   top_y = min([pt[1] for pt in top_points if x - 15 <= pt[0] < x + region_width * scaling_factor + 15], default=None)
                   bottom_y = max([pt[1] for pt in bottom_points if x - 15 <= pt[0] < x + region_width * scaling_factor + 15], default=None)

                   if top_y is not None and bottom_y is not None:
                       height = abs(bottom_y - top_y) / scaling_factor
                       midpoint_y = (top_y + bottom_y) // 2
                       measurement_points.append((x // scaling_factor + region_width // 2, top_y // scaling_factor, bottom_y // scaling_factor, height, midpoint_y // scaling_factor))
                   else:
                       if measurement_points:
                           prev_x, prev_top_y, prev_bottom_y, prev_height, prev_midpoint_y = measurement_points[-1]
                           measurement_points.append((x // scaling_factor + region_width // 2, prev_top_y, prev_bottom_y, prev_height, prev_midpoint_y))

           return measurement_points

       def get_colored_overlay(self, original_image, predicted_mask):
           colored_mask = cv2.cvtColor(predicted_mask.squeeze() * 255, cv2.COLOR_GRAY2RGB)
           overlay = cv2.addWeighted(original_image, 0.5, colored_mask, 1, 0)
           return overlay

       def get_mask_with_measurements(self, predicted_mask, measurement_points):
           mask_with_points = cv2.cvtColor(predicted_mask.squeeze() * 255, cv2.COLOR_GRAY2RGB)

           for (x, top_y, bottom_y, height, midpoint_y) in measurement_points:
               cv2.circle(mask_with_points, (x, top_y), 2, (255, 0, 0), -1)
               cv2.circle(mask_with_points, (x, bottom_y), 2, (255, 0, 0), -1)
               cv2.line(mask_with_points, (x, top_y), (x, bottom_y), (255, 255, 255), 1)
               cv2.putText(mask_with_points, f"{int(height)}", (x + 5,

 midpoint_y), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)

           return mask_with_points

   def main(args=None):
       rclpy.init(args=args)
       image_subscriber = ImageSubscriber()
       rclpy.spin(image_subscriber)
       image_subscriber.destroy_node()
       rclpy.shutdown()

   if __name__ == '__main__':
       main()
   ```

### **Explanation:**

- **Model Integration**: The model is loaded in the `__init__` function, and the `listener_callback` function now processes each received image through the ML model.
- **Image Naming**: The original image's filename is extracted from the message's `header.frame_id`, ensuring the output files maintain consistency with the input filenames.
- **Image Saving**: Images are saved in a subfolder named after the original image filename with `_results` appended.

For a more detailed explanation of how this script works and what it does (its different components) go to the following page: 

### **Step 2: Update the `setup.py`**

Ensure that the `setup.py` file correctly points to the updated subscriber script:

1. **Open the `setup.py` File**:
   ```bash
   cd ~/ros2_ws_test/src/image_processor_test
   gedit setup.py
   ```

2. **Ensure the `entry_points` Section Includes the Subscriber**:
   ```python
   entry_points={
       'console_scripts': [
           'image_publisher = image_processor_test.scripts.image_publisher:main',
           'image_subscriber = image_processor_test.scripts.image_subscriber:main',
       ],
   },
   ```

### **Step 3: Build the Workspace**

1. **Return to the Workspace Root**:
   ```bash
   cd ~/ros2_ws_test
   ```

2. **Build the Workspace**:
   ```bash
   colcon build --packages-select image_processor_test
   ```

3. **Source the Workspace**:
   ```bash
   source install/setup.bash
   ```

### **Step 4: Test the System**

1. **Run the Image Publisher Node**:
   ```bash
   ros2 run image_processor_test image_publisher
   ```

2. **Run the Image Subscriber Node**:
   ```bash
   ros2 run image_processor_test image_subscriber
   ```
